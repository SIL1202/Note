## GRU (Gated Recurrent Unit)

門控循環單元（GRU）是 LSTM 的簡化版本，沒有額外的記憶單元 $c$，而是直接用隱藏狀態來保存資訊。它只使用兩個閘門，但仍能有效解決梯度消失問題。

核心組成：

1. 更新閘門（Update Gate, z）:

   - 控制新的隱藏狀態 $h_t$ 中，應保留多少就資訊 $h_{t-1}$，以及加入多少新資訊（候選隱藏狀態 $\tilde{h}_t$）。

   - 公式：

     $z_t = 𝜎(W_z \cdot [h_{t-1}, x_t])$

   -  如果 $z_t$ 越接近 1，代表更多保留過去的資訊；越接近 0，則代表傾向更新為新資訊。

2. 重置閘門 (Reset Gate, r)：

   - 決定在計算候選隱藏狀態$\tilde{h_t}$ 時，應該忘記多少就資訊。

   - 公式：

     $r_t = 𝜎(W_r \cdot [h_{t-1}, x_t])$

   - 如果 $r_t$ 接近 0 ，會大幅忽略舊的隱藏狀態，只關注當前輸入 ; 適合捕捉短期依賴。

   

   **候選隱藏狀態計算：**

   ​	$\tilde{h_t} = tanh(W_h [x_t, r_t * h_{t-1}])$

   **最終隱藏狀態更新：**

   ​	$h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h_t}$

   

   > 更新閘門就是在調配舊記憶和新訊息的比例。
   >
   > 重置閘門則決定再產生新訊息時，要不要參考過去的記憶。

   

   **特點**：

   - 結構比 LSTM 簡單，沒有單獨的記憶單元 $c$。
   - 參數更少、計算更快，適合訓練資料量有限或需要高效能的情境。
   - 在許多任務上表現與 LSTM 相當，甚至更佳。
